<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Coffee Bean Roast Level Detector</title>
    <script src="https://cdn.jsdelivr.net/npm/onnxjs/dist/onnx.min.js"></script>
    <style>
        body {
            font-family: Arial, sans-serif;
            text-align: center;
            padding: 20px;
            background-color: #f0f0f0;
        }
        video, canvas {
            border: 1px solid black;
            max-width: 100%;
        }
        #output, #summary {
            margin-top: 20px;
        }
        table {
            margin: 20px auto;
            border-collapse: collapse;
        }
        th, td {
            border: 1px solid black;
            padding: 8px;
        }
        button {
            padding: 10px 20px;
            margin: 10px;
            background-color: #4CAF50;
            color: white;
            border: none;
            cursor: pointer;
        }
        button:hover {
            background-color: #45a049;
        }
    </style>
</head>
<body>
    <h1>Coffee Bean Roast Level Detector</h1>
    <p>Upload an image or use your camera to detect coffee bean roast levels.</p>
    <input type="file" id="imageInput" accept="image/*">
    <button id="startCamera">Use Camera</button>
    <button id="capture" style="display:none;">Capture Photo</button>
    <br><br>
    <video id="video" width="640" height="640" autoplay style="display:none;"></video>
    <canvas id="canvas" width="640" height="640"></canvas>
    <div id="summary"></div>
    <div id="output"></div>

    <script>
        async function loadModel() {
            const session = new onnx.InferenceSession();
            await session.loadModel("best.onnx"); // Update path if hosted elsewhere
            return session;
        }

        async function preprocessImage(imageData) {
            const canvas = document.createElement("canvas");
            canvas.width = 640;
            canvas.height = 640;
            const ctx = canvas.getContext("2d");
            ctx.drawImage(imageData, 0, 0, 640, 640);
            const imageDataObj = ctx.getImageData(0, 0, 640, 640);
            const { data } = imageDataObj;
            const floatData = new Float32Array(640 * 640 * 3);
            for (let i = 0, j = 0; i < data.length; i += 4, j += 3) {
                floatData[j] = data[i] / 255.0;     // R
                floatData[j + 1] = data[i + 1] / 255.0; // G
                floatData[j + 2] = data[i + 2] / 255.0; // B
            }
            return { tensor: new onnx.Tensor(floatData, "float32", [1, 3, 640, 640]), canvas };
        }

        function nonMaxSuppression(boxes, scores, iouThreshold = 0.5) {
            const selectedIndices = [];
            const sortedIndices = scores
                .map((score, i) => ({ score, index: i }))
                .sort((a, b) => b.score - a.score)
                .map(item => item.index);

            while (sortedIndices.length > 0) {
                const current = sortedIndices.shift();
                selectedIndices.push(current);
                const currentBox = boxes[current];
                sortedIndices.filter(i => {
                    const box = boxes[i];
                    const x1 = Math.max(currentBox[0], box[0]);
                    const y1 = Math.max(currentBox[1], box[1]);
                    const x2 = Math.min(currentBox[2], box[2]);
                    const y2 = Math.min(currentBox[3], box[3]);
                    const w = Math.max(0, x2 - x1);
                    const h = Math.max(0, y2 - y1);
                    const intersection = w * h;
                    const areaCurrent = (currentBox[2] - currentBox[0]) * (currentBox[3] - currentBox[1]);
                    const areaBox = (box[2] - box[0]) * (box[3] - box[1]);
                    const iou = intersection / (areaCurrent + areaBox - intersection);
                    return iou <= iouThreshold;
                });
            }
            return selectedIndices;
        }

        async function runInference(session, tensor) {
            const outputMap = await session.run([tensor]);
            const output = outputMap.values().next().value.data;
            return output;
        }

        function parseYOLOv5Output(output, confThreshold = 0.25) {
            const boxes = [];
            const scores = [];
            const classes = [];
            const stride = 7; // Adjust based on your model's output format (typically 5 + num_classes)
            const numClasses = 4; // bad, dark_green, light, medium_green
            for (let i = 0; i < output.length; i += stride) {
                const confidence = output[i + 4];
                if (confidence > confThreshold) {
                    const classScores = output.slice(i + 5, i + 5 + numClasses);
                    const classId = classScores.indexOf(Math.max(...classScores));
                    boxes.push([
                        output[i] * 640, // x_center
                        output[i + 1] * 640, // y_center
                        output[i + 2] * 640, // width
                        output[i + 3] * 640 // height
                    ]);
                    scores.push(confidence);
                    classes.push(classId);
                }
            }
            const selectedIndices = nonMaxSuppression(boxes.map(box => [
                box[0] - box[2] / 2, // x1
                box[1] - box[3] / 2, // y1
                box[0] + box[2] / 2, // x2
                box[1] + box[3] / 2 // y2
            ]), scores);
            return selectedIndices.map(i => ({
                box: boxes[i],
                score: scores[i],
                classId: classes[i]
            }));
        }

        function drawBoundingBoxes(canvas, predictions, classNames) {
            const ctx = canvas.getContext("2d");
            ctx.font = "16px Arial";
            predictions.forEach(({ box, score, classId }) => {
                const [x, y, w, h] = box;
                ctx.strokeStyle = "red";
                ctx.lineWidth = 2;
                ctx.strokeRect(x - w / 2, y - h / 2, w, h);
                ctx.fillStyle = "red";
                ctx.fillText(`${classNames[classId]} (${(score * 100).toFixed(2)}%)`, x - w / 2, y - h / 2 - 5);
            });
            return canvas;
        }

        function createSummaryTable(predictions, classNames) {
            const counts = classNames.map(() => 0);
            predictions.forEach(({ classId }) => counts[classId]++);
            let table = "<table><tr><th>Class</th><th>Count</th></tr>";
            classNames.forEach((name, i) => {
                table += `<tr><td>${name}</td><td>${counts[i]}</td></tr>`;
            });
            table += "</table>";
            return table;
        }

        async function processImage(imageData) {
            const canvas = document.getElementById("canvas");
            const outputDiv = document.getElementById("output");
            const summaryDiv = document.getElementById("summary");
            outputDiv.innerText = "Processing...";
            const session = await loadModel();
            const { tensor, canvas: tempCanvas } = await preprocessImage(imageData);
            const predictionsRaw = await runInference(session, tensor);
            const classNames = ["bad", "dark_green", "light", "medium_green"]; // Update based on your data.yaml
            const predictions = parseYOLOv5Output(predictionsRaw);
            const ctx = canvas.getContext("2d");
            ctx.drawImage(tempCanvas, 0, 0);
            drawBoundingBoxes(canvas, predictions, classNames);
            summaryDiv.innerHTML = createSummaryTable(predictions, classNames);
            outputDiv.innerText = "Prediction complete! See the labeled image and summary table.";
        }

        // File upload handler
        document.getElementById("imageInput").addEventListener("change", async (event) => {
            const file = event.target.files[0];
            if (!file) return;
            const img = new Image();
            img.src = URL.createObjectURL(file);
            img.onload = () => processImage(img);
        });

        // Camera support
        const video = document.getElementById("video");
        const captureButton = document.getElementById("capture");
        document.getElementById("startCamera").addEventListener("click", async () => {
            try {
                const stream = await navigator.mediaDevices.getUserMedia({ video: { facingMode: "environment" } });
                video.srcObject = stream;
                video.style.display = "block";
                captureButton.style.display = "inline-block";
            } catch (err) {
                document.getElementById("output").innerText = "Error accessing camera: " + err.message;
            }
        });

        captureButton.addEventListener("click", () => {
            const canvas = document.createElement("canvas");
            canvas.width = 640;
            canvas.height = 640;
            canvas.getContext("2d").drawImage(video, 0, 0, 640, 640);
            video.style.display = "none";
            captureButton.style.display = "none";
            video.srcObject.getTracks().forEach(track => track.stop());
            processImage(canvas);
        });
    </script>
</body>
</html>